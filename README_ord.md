# Awesome Image Editing 
This is the github repository of our work "A Survey of Multimodal-Guided Image Editing with Text-to-Image Diffusion Models". 
<!-- We categorize the reviewed papers by their editing scenario, and illustrate their inversion and editing algorithms. -->


## Editing Tasks Discussed in Our Survey
<p align="center">
  <img src="./images/editing_task.jpg" alt="image" style="width:800px;">
</p>

## Unified Framework
<p align="center">
  <img src="./images/unified_framework.jpg" alt="image" style="width:800px;">
</p>



## Table of contents
Content-Aware Editing
- [Object Manipulation + Attribute Manipulation](#object-manipulation-and-attribute-manipulation)
- [Attribute Manipulation](#attribute-manipulation)
- [Spatial Transformation](#spatial-transformation)
- [Inpainting](#inpainting)
- [Style Change](#style-change)
- [Image Translation](#image-translation)

<br>

Content-Free Editing
- [Subject-Driven Customization](#subject-driven-customization)
- [Attribute-Driven Customization](#attribute-driven-customization)

<br>

Experiment and Data
- [Data](#data)



<br>

## Object Manipulation and Attribute Manipulation:
### 1. Training-Free Approaches 
  [ğŸ“„ UniTune: Text-Driven Image Editing by Fine Tuning a Diffusion Model on a Single Image](https://arxiv.org/abs/2210.09477) | ğŸ“– TOG 2023 | ğŸ”€ $F_{inv}^T+F_{edit}^{Norm}$ | [ğŸŒ Code]() 
   
  [ğŸ“„ Highly Personalized Text Embedding for Image Manipulation by Stable Diffusion](https://arxiv.org/abs/2303.08767) | ğŸ“– Arxiv 2023 | ğŸ”€ $F_{inv}^T+F_{edit}^{Norm}$| [ğŸŒ Code](https://github.com/HiPer0/HiPer) 

  [ğŸ“„ Imagic: Text-Based Real Image Editing with Diffusion Models](https://arxiv.org/abs/2210.09276) | ğŸ“– CVPR 2023 | ğŸ”€ $F_{inv}^T+F_{edit}^{Blend}$ | [ğŸŒ Code] 

  [ğŸ“„ Forgedit: Text Guided Image Editing via Learning and Forgetting](https://arxiv.org/abs/2309.10556) | ğŸ“– Arxiv 2023 | ğŸ”€ $F_{inv}^T+F_{edit}^{Blend}$ | [ğŸŒ Code](https://github.com/witcherofresearch/Forgedit) 
   
  [ğŸ“„ Doubly Abductive Counterfactual Inference for Text-based Image Editing](https://arxiv.org/abs/2403.02981) | ğŸ“– CVPR 2024 | ğŸ”€ $F_{inv}^T+F_{edit}^{Blend}$ | [ğŸŒ Code](https://github.com/xuesong39/DAC) 

  [ğŸ“„ SINE: Sinle Image Editing with Text-to-Image Diffusion Models](https://arxiv.org/abs/2212.04489) | ğŸ“– CVPR 2023 | ğŸ”€ $F_{inv}^T+F_{edit}^{Score}$ | [ğŸŒ Code](https://github.com/zhang-zx/SINE) 


  [ğŸ“„ EDICT: Exact Diffusion Inversion via Coupled Transformations](https://arxiv.org/abs/2211.12446) | ğŸ“– CVPR 2023  | ğŸ”€ $F_{inv}^F+F_{edit}^{Norm}$ | [ğŸŒ Code](https://github.com/salesforce/EDICT) 

  [ğŸ“„ Exact Diffusion Inversion via Bi-directional Integration Approximation](https://arxiv.org/abs/2307.10829) | ğŸ“– Arxiv 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Norm}$ | [ğŸŒ Code]() 

  [ğŸ“„ Effective Real Image Editing with Accelerated Iterative Diffusion Inversion](https://arxiv.org/abs/2309.04907) | ğŸ“– ICCV 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Norm}$ | [ğŸŒ Code]() 



  [ğŸ“„ Null-text Inversion for Editing Real Images using Guided Diffusion Models](https://arxiv.org/abs/2211.09794) | ğŸ“– CVPR 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Attn}$ | [ğŸŒ Code](https://github.com/google/prompt-to-prompt/#null-text-inversion-for-editing-real-images) 

  [ğŸ“„ Negative-prompt Inversion: Fast Image Inversion for Editing with Text-guided Diffusion Models](https://arxiv.org/abs/2305.16807) | ğŸ“– Arxiv 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Attn}$ | [ğŸŒ Code]() 

  [ğŸ“„ProxEdit: Improving Tuning-Free Real Image Editing with Proximal Guidance](https://arxiv.org/pdf/2306.05414) | ğŸ“– WACV 2024 | ğŸ”€ $F_{inv}^F+F_{edit}^{Attn}$ | [ğŸŒ Code](https://github.com/phymhan/prompt-to-prompt) 
   
  [ğŸ“„ Fixed-point Inversion for Text-to-image diffusion models](https://arxiv.org/abs/2312.12540v1) | ğŸ“– Arxiv 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Attn}$ | [ğŸŒ Code]() 

  [ğŸ“„PnP Inversion: Boosting Diffusion-based Editing with 3 Lines of Code](https://arxiv.org/abs/2310.01506) | ğŸ“– ICLR 2024 | ğŸ”€ $F_{inv}^F+F_{edit}^{Attn}$| [ğŸŒ Code](https://github.com/cure-lab/PnPInversion) 

  [ğŸ“„ Dynamic Prompt Learning: Addressing Cross-Attention Leakage for Text-Based Image Editing](https://arxiv.org/abs/2309.15664) | ğŸ“– NeurIPS 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Attn}$ | [ğŸŒ Code]() 

  [ğŸ“„ An Edit Friendly DDPM Noise Space: Inversion and Manipulations](https://arxiv.org/abs/2304.06140) | ğŸ“– CVPR 2024 | ğŸ”€ $F_{inv}^F+F_{edit}^{Attn}$ | [ğŸŒ Code](https://github.com/inbarhub/DDPM_inversion) 

  [ğŸ“„ Prompt-to-Prompt Image Editing with Cross-Attention Control](https://arxiv.org/abs/2208.01626) | ğŸ“– ICLR 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Attn}$ | [ğŸŒ Code](https://github.com/google/prompt-to-prompt/#null-text-inversion-for-editing-real-images) 

  [ğŸ“„ Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation](https://arxiv.org/abs/2211.12572) | ğŸ“– CVPR 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Attn}$ | [ğŸŒ Code](https://github.com/MichalGeyer/plug-and-play) 

  [ğŸ“„ Towards Understanding Cross and Self-Attention in Stable Diffusion for Text-Guided Image Editing](https://arxiv.org/abs/2403.03431) | ğŸ“– CVPR 2024 | ğŸ”€ $F_{inv}^F+F_{edit}^{Attn}$ | [ğŸŒ Code](https://github.com/alibaba/EasyNLP/tree/master/diffusion/FreePromptEditing) 

  [ğŸ“„ StyleDiffusion: Prompt-Embedding Inversion for Text-Based Editing](https://arxiv.org/abs/2303.15649) | ğŸ“– Arxiv 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Attn}$ | [ğŸŒ Code](https://github.com/sen-mao/StyleDiffusion) 


  
  [ğŸ“„ Prompt Tuning Inversion for Text-Driven Image Editing Using Diffusion Models](https://arxiv.org/abs/2305.04441) | ğŸ“– ICCV 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Blend}$ | [ğŸŒ Code]() 

  [ğŸ“„ Object-aware Inversion and Reassembly for Image Editing](https://arxiv.org/abs/2310.12149) | ğŸ“– ICLR 2024 | ğŸ”€ $F_{inv}^F+F_{edit}^{Blend}$ | [ğŸŒ Code](https://github.com/aim-uofa/OIR) 

  [ğŸ“„ DiffEdit: Diffusion-based semantic image editing with mask guidance](https://arxiv.org/abs/2210.11427) | ğŸ“– ICLR 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Blend}$ | [ğŸŒ Code]() 

  [ğŸ“„ PFB-Diff: Progressive Feature Blending Diffusion for Text-driven Image Editing](https://arxiv.org/abs/2306.16894) | ğŸ“– Arxiv 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Blend}$ | [ğŸŒ Code]() 

  [ğŸ“„ Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models](https://arxiv.org/abs/2212.08698) | ğŸ“– CVPR 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Blend}$| [ğŸŒ Code](https://github.com/UCSB-NLP-Chang/DiffusionDisentanglement) 


  [ğŸ“„ Noise Map Guidance: Inversion with Spatial Context for Real Image Editing](https://arxiv.org/abs/2402.04625) | ğŸ“– ICLR 2024 | ğŸ”€ $F_{inv}^F+F_{edit}^{Score}$ | [ğŸŒ Code](https://github.com/hansam95/NMG) 
  
  [ğŸ“„ pix2pix-zero](https://arxiv.org/abs/2302.03027) | ğŸ“– SIGGRAPH 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Score}$ | [ğŸŒ Code](https://github.com/pix2pixzero/pix2pix-zero) 

  [ğŸ“„ SEGA: Instructing Diffusion using Semantic Dimensions](https://export.arxiv.org/abs/2301.12247v1) | ğŸ“– NeurIPS 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Score}$ | [ğŸŒ Code]() 

  [ğŸ“„ The Stable Artist: Steering Semantics in Diffusion Latent Space](https://arxiv.org/abs/2212.06013) | ğŸ“– Arxiv 2022 | ğŸ”€ $F_{inv}^F+F_{edit}^{Score}$ | [ğŸŒ Code]() 

  [ğŸ“„ LEDITS: Real Image Editing with DDPM Inversion and Semantic Guidance](https://arxiv.org/abs/2307.00522) | ğŸ“– Arxiv 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Score}$ | [ğŸŒ Code](https://huggingface.co/spaces/editing-images/ledits/tree/main) 

  [ğŸ“„ LEDITS++: Limitless Image Editing using Text-to-Image Models](https://arxiv.org/abs/2311.16711) | ğŸ“– CVPR 2024 | ğŸ”€ $F_{inv}^F+F_{edit}^{Score}$ | [ğŸŒ Code](https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines/ledits_pp) 

  [ğŸ“„ Magicremover: Tuning-free Text-guided Image inpainting with Diffusion Models](https://arxiv.org/abs/2310.02848) | ğŸ“– ICLR 2024 | ğŸ”€ $F_{inv}^F+F_{edit}^{Score}$ | [ğŸŒ Code]() 


  [ğŸ“„ Region-Aware Diffusion for Zero-shot Text-driven Image Editing](https://arxiv.org/abs/2302.11797) | ğŸ“– Arxiv 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Optim}$ | [ğŸŒ Code]() 

  [ğŸ“„ Delta Denoising Score](https://arxiv.org/abs/2304.07090) | ğŸ“– ICCV 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Optim}$ | [ğŸŒ Code](https://github.com/google/prompt-to-prompt/blob/main/DDS_zeroshot.ipynb) 

  [ğŸ“„ Contrastive Denoising Score for Text-guided Latent Diffusion Image Editing](https://arxiv.org/abs/2311.18608) | ğŸ“– CVPR 2024 | ğŸ”€ $F_{inv}^F+F_{edit}^{Optim}$ | [ğŸŒ Code](https://github.com/HyelinNAM/ContrastiveDenoisingScore) 

  [ğŸ“„ Ground-A-Score: Scaling Up the Score Distillation for Multi-Attribute Editing](https://arxiv.org/abs/2403.13551) | ğŸ“– Arxiv 2024 | ğŸ”€ $F_{inv}^F+F_{edit}^{Optim}$ | [ğŸŒ Code](https://github.com/Ground-A-Score/Ground-A-Score/) 



  [ğŸ“„ Custom-Edit: Text-Guided Image Editing with Customized Diffusion Models](https://arxiv.org/abs/2305.15779) | ğŸ“– CVPR 2023 | ğŸ”€ $F_{inv}^T+F_{inv}^F+F_{edit}^{Attn}$ | [ğŸŒ Code]() 

  [ğŸ“„ Photoswap: Personalized Subject Swapping in Images](https://arxiv.org/abs/2305.18286) | ğŸ“– NeurIPS 2023 | ğŸ”€ $F_{inv}^T+F_{inv}^F+F_{edit}^{Attn}$ | [ğŸŒ Code](https://github.com/eric-ai-lab/photoswap) 

  [ğŸ“„ DreamEdit: Subject-driven Image Editing](https://arxiv.org/abs/2306.12624) | ğŸ“– TMLR 2023 |ğŸ”€ $F_{inv}^T+F_{inv}^F+F_{edit}^{Blend}$ | [ğŸŒ Code](https://github.com/DreamEditBenchTeam/DreamEdit) 




### 2. Training-Based Approaches


  [ğŸ“„ InstructPix2Pix: Learning to Follow Image Editing Instructions](https://arxiv.org/abs/2211.09800) | ğŸ“– CVPR 2023 | [ğŸŒ Code](https://github.com/timothybrooks/instruct-pix2pix) 

  [ğŸ“„ MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing](https://arxiv.org/abs/2306.10012) | ğŸ“– NeurIPS 2023 | [ğŸŒ Code](https://github.com/OSU-NLP-Group/MagicBrush) 

  [ğŸ“„ HIVE: Harnessing Human Feedback for Instructional Visual Editing](https://arxiv.org/abs/2303.09618) | ğŸ“– Arxiv 2023 | [ğŸŒ Code](https://github.com/salesforce/HIVE) 


  [ğŸ“„ Emu Edit: Precise Image Editing via Recognition and Generation Tasks](https://arxiv.org/abs/2311.10089) | ğŸ“– Arxiv 2023 |  [ğŸŒ Code](https://emu-edit.metademolab.com/) 

  [ğŸ“„ GUIDING INSTRUCTION-BASED IMAGE EDITING VIA MULTIMODAL LARGE LANGUAGE MODELS](https://arxiv.org/abs/2309.17102) | ğŸ“– ICLR 2024 | [ğŸŒ Code](https://mllm-ie.github.io/) 

  [ğŸ“„ SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models](https://arxiv.org/abs/2312.06739) | ğŸ“–CVPR 2024 | [ğŸŒ Code](https://github.com/TencentARC/SmartEdit) 

  [ğŸ“„ Referring Image Editing: Object-level Image Editing via Referring Expressions](https://openaccess.thecvf.com/content/CVPR2024/html/Liu_Referring_Image_Editing_Object-level_Image_Editing_via_Referring_Expressions_CVPR_2024_paper.html) | ğŸ“–CVPR 2024  | [ğŸŒ Code]() 


<br>

## Attribute Manipulation:
### 1. Training-Free Approaches

  [ğŸ“„ KV Inversion: KV Embeddings Learning for Text-Conditioned Real Image Action Editing](https://arxiv.org/abs/2309.16608) | ğŸ“– PRCV 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Attn}$ | [ğŸŒ Code]() 

  [ğŸ“„ Localizing Object-level Shape Variations with Text-to-Image Diffusion Models](https://arxiv.org/abs/2303.11306) | ğŸ“– ICCV 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Attn}$ | [ğŸŒ Code](https://github.com/orpatashnik/local-prompt-mixing) 

  [ğŸ“„ MasaCtrl: Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing](https://arxiv.org/abs/2304.08465) | ğŸ“– ICCV 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Attn}$ | [ğŸŒ Code](https://github.com/TencentARC/MasaCtrl) 
   
  [ğŸ“„ Tuning-Free Inversion-Enhanced Control for Consistent Image Editing](https://arxiv.org/abs/2312.14611) | ğŸ“– AAAI 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Attn}$ | [ğŸŒ Code]() 

  [ğŸ“„ Cross-Image Attention for Zero-Shot Appearance Transfer](https://arxiv.org/abs/2311.03335) | ğŸ“– SIGGRAPH 2024 | ğŸ”€ $F_{inv}^F+F_{edit}^{Attn}$ | [ğŸŒ Code](https://github.com/garibida/cross-image-attention) 



### 2. Training-Based Approaches

<br>


## Spatial Transformation:
### 1. Training-Free Approaches

  [ğŸ“„ DesignEdit: Multi-Layered Latent Decomposition and Fusion for Unified & Accurate Image Editing](https://arxiv.org/abs/2403.14487) | ğŸ“– Arxiv 2024 | ğŸ”€ $F_{inv}^F+F_{edit}^{Attn}$ | [ğŸŒ Code](https://github.com/design-edit/DesignEdit) 
 
  [ğŸ“„ Diffusion Self-Guidance for Controllable Image Generation](https://arxiv.org/abs/2306.00986) | ğŸ“– NeurIPS 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Score}$ | [ğŸŒ Code](https://dave.ml/selfguidance/) 

  [ğŸ“„ DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models](https://arxiv.org/abs/2307.02421) | ğŸ“– ICLR 2024 | ğŸ”€ $F_{inv}^F+F_{edit}^{Score}$ | [ğŸŒ Code](https://github.com/MC-E/DragonDiffusion) 

 


  [ğŸ“„ DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing](https://arxiv.org/abs/2306.14435) | ğŸ“– ICLR 2024 | ğŸ”€ $F_{inv}^T+F_{inv}^F+F_{edit}^{Optim}$ | [ğŸŒ Code](https://github.com/Yujun-Shi/DragDiffusion) 

  [ğŸ“„ DiffEditor: Boosting Accuracy and Flexibility on Diffusion-based Image Editing](https://arxiv.org/abs/2402.02583) | ğŸ“– ICLR 2024 | ğŸ”€ $F_{inv}^T+F_{inv}^F+F_{edit}^{Score}$ | [ğŸŒ Code](https://github.com/MC-E/DragonDiffusion) 


### 2. Training-Based Approaches




<br>

## Inpainting:
### 1. Training-Free Approaches
  [ğŸ“„ HD-Painter: High-Resolution and Prompt-Faithful Text-Guided Image Inpainting with Diffusion Models](https://arxiv.org/abs/2312.14091) | ğŸ“– Arxiv 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Attn}$  | [ğŸŒ Code](https://github.com/Picsart-AI-Research/HD-Painter) 

  [ğŸ“„ TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition](https://arxiv.org/abs/2307.12493) | ğŸ“– ICCV 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Blend}$ | [ğŸŒ Code](https://github.com/Shilin-LU/TF-ICON) 

  [ğŸ“„ Blended Latent Diffusion](https://arxiv.org/abs/2206.02779) | ğŸ“– TOG 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Blend}$  | [ğŸŒ Code](https://github.com/omriav/blended-latent-diffusion) 

  [ğŸ“„ High-Resolution Image Editing via Multi-Stage Blended Diffusion](https://arxiv.org/abs/2210.12965) | ğŸ“– Arxiv 2022 | ğŸ”€ $F_{inv}^F+F_{edit}^{Blend}$  | [ğŸŒ Code](https://github.com/pfnet-research/multi-stage-blended-diffusion) 

  [ğŸ“„ Differential Diffusion: Giving Each Pixel Its Strength](https://arxiv.org/abs/2306.00950) | ğŸ“– Arxiv 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Blend}$  | [ğŸŒ Code](https://github.com/exx8/differential-diffusion) 
   
  [ğŸ“„ Tuning-Free Image Customization with Image and Text Guidance](https://arxiv.org/abs/2403.12658) | ğŸ“– CVPR 2024 | ğŸ”€ $F_{inv}^F+F_{edit}^{Blend}$  | [ğŸŒ Code]() 


  <!-- [ğŸ“„ DreamEdit]() | [ğŸ“– ] | [Inversion+Editing] | [ğŸŒ Code]()  -->




### 2. Training-Based Approaches


  [ğŸ“„ Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting](https://arxiv.org/pdf/2212.06909) | ğŸ“– CVPR 2024| [ğŸŒ Code](https://imagen.research.google/editor/) 

  [ğŸ“„ SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model](https://arxiv.org/abs/2212.05034) | ğŸ“– CVPR 2023 | [ğŸŒ Code]() 


  [ğŸ“„ A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting](https://arxiv.org/abs/2312.03594) | ğŸ“– Arxiv 2023 | [ğŸŒ Code](https://github.com/open-mmlab/PowerPaint) 

  [ğŸ“„ Paint by Example: Exemplar-based Image Editing with Diffusion Models](https://arxiv.org/abs/2211.13227) | ğŸ“– CVPR 2023 | [ğŸŒ Code](https://github.com/Fantasy-Studio/Paint-by-Example) 

  [ğŸ“„ ObjectStitch: Object Compositing with Diffusion Model](https://arxiv.org/abs/2212.00932) | ğŸ“– CVPR 2023 | [ğŸŒ Code]() 
   
  [ğŸ“„ Reference-based Image Composition with Sketch via Structure-aware Diffusion Model](https://arxiv.org/abs/2304.09748) | ğŸ“– CVPR 2023  | [ğŸŒ Code]() 


  [ğŸ“„ Paste, Inpaint and Harmonize via Denoising: Subject-Driven Image Editing with Pre-Trained Diffusion Model](https://arxiv.org/abs/2306.07596) | ğŸ“– ICASSP 2024 | [ğŸŒ Code](https://sites.google.com/view/phd-demo-page) 

  [ğŸ“„ AnyDoor: Zero-shot Object-level Image Customization](https://arxiv.org/abs/2307.09481) | ğŸ“– CVPR 2024 | [ğŸŒ Code](https://github.com/ali-vilab/AnyDoor) 


<br>


## Style Change:
### 1. Training-Free Approaches
  [ğŸ“„ Inversion-Based Style Transfer with Diffusion Models](https://arxiv.org/abs/2211.13203) | ğŸ“– CVPR 2023 | ğŸ”€ $F_{inv}^T+F_{inv}^F+F_{edit}^{Attn}$  | [ğŸŒ Code](https://github.com/zyxElsa/InST) 

  [ğŸ“„ Style Injection in Diffusion: A Training-free Approach for Adapting Large-scale Diffusion Models for Style Transfer](https://arxiv.org/abs/2312.09008) | ğŸ“– CVPR 2024 | ğŸ”€ $F_{inv}^F+F_{edit}^{Attn}$  | [ğŸŒ Code](https://github.com/jiwoogit/StyleID) 

  [ğŸ“„ Zâˆ—: Zero-shot Style Transfer via Attention Rearrangement](https://arxiv.org/abs/2311.16491) | ğŸ“– Arxiv 2023 | ğŸ”€ $F_{inv}^F+F_{edit}^{Attn}$ | [ğŸŒ Code]() 


### 2. Training-Based Approaches

<br>


## Image Translation:
### 1. Training-Free Approaches
  [ğŸ“„ FreeControl: Training-Free Spatial Control of Any Text-to-Image Diffusion Model with Any Condition](https://arxiv.org/abs/2312.07536) | ğŸ“– CVPR 2024 | [ğŸŒ Code](https://github.com/genforce/freecontrol) 

### 2. Training-Based Approaches
  [ğŸ“„ Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543) | ğŸ“– ICCV 2023 | [ğŸŒ Code]() 

  [ğŸ“„ T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.08453) | ğŸ“– AAAI 2024 | [ğŸŒ Code](https://github.com/TencentARC/T2I-Adapter) 
   
  [ğŸ“„ SCEdit: Efficient and Controllable Image Diffusion Generation via Skip Connection Editing](https://arxiv.org/abs/2312.11392) | ğŸ“– CVPR 2024 | [ğŸŒ Code](https://github.com/ali-vilab/SCEdit) 


  [ğŸ“„ Cocktail: Mixing Multi-Modality Controls for Text-Conditional Image Generation](https://arxiv.org/abs/2306.00964) | ğŸ“– NeurIPS 2023 | [ğŸŒ Code](https://github.com/mhh0318/Cocktail) 

  [ğŸ“„ Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Model](https://arxiv.org/abs/2305.16322) | ğŸ“– NeurIPS 2023 | [ğŸŒ Code](https://github.com/ShihaoZhaoZSH/Uni-ControlNet) 

  [ğŸ“„ CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation](https://arxiv.org/abs/2310.13165) | ğŸ“– NeurIPS 2023 | [ğŸŒ Code](https://github.com/sled-group/CycleNet) 

  [ğŸ“„ One-Step Image Translation with Text-to-Image Models](https://arxiv.org/abs/2403.12036) | ğŸ“– Arxiv 2024] | [ğŸŒ Code](https://github.com/GaParmar/img2img-turbo) 

<br>

## Subject-Driven Customization:
### 1. Training-Free Approaches
  [ğŸ“„ An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion](https://arxiv.org/abs/2208.01618) | ğŸ“– ICLR 2023 | ğŸ”€ $F_{inv}^T+F_{edit}^{Norm}$ | [ğŸŒ Code](https://github.com/rinongal/textual_inversion) 
   
  [ğŸ“„ DreamArtist: Towards Controllable One-Shot Text-to-Image Generation via Positive-Negative Prompt-Tuning](https://arxiv.org/abs/2211.11337) | ğŸ“– Arxiv 2022 | ğŸ”€ $F_{inv}^T+F_{edit}^{Norm}$ | [ğŸŒ Code]() 

  <!-- [ğŸ“„ Cones2]() | [ğŸ“– ] | [Inversion+Editing] | [ğŸŒ Code]()  -->
   
  [ğŸ“„ P+: Extended Textual Conditioning in Text-to-Image Generation](https://arxiv.org/abs/2303.09522) | ğŸ“– Arxiv 2023 | ğŸ”€ $F_{inv}^T+F_{edit}^{Norm}$ | [ğŸŒ Code](https://prompt-plus.github.io/) 

  [ğŸ“„ A Neural Space-Time Representation for Text-to-Image Personalization](https://arxiv.org/abs/2305.15391) | ğŸ“– TOG 2023 | ğŸ”€ $F_{inv}^T+F_{edit}^{Norm}$ | [ğŸŒ Code](https://github.com/NeuralTextualInversion/NeTI) 
   
  [ğŸ“„ DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation](https://arxiv.org/abs/2208.12242) | ğŸ“– CVPR 2023 | ğŸ”€ $F_{inv}^T+F_{edit}^{Norm}$ | [ğŸŒ Code](https://dreambooth.github.io/) 


  [ğŸ“„A Data Perspective on Enhanced Identity Preservation for Diffusion Personalization](https://arxiv.org/abs/2311.04315) | ğŸ“– ICLR 2024 | ğŸ”€ $F_{inv}^T+F_{edit}^{Norm}$ | [ğŸŒ Code]() 
   
  [ğŸ“„ FaceChain-SuDe: Building Derived Class to Inherit Category Attributes for One-shot Subject-Driven Generation](https://arxiv.org/abs/2403.06775) | ğŸ“– CVPR 2024 | ğŸ”€ $F_{inv}^T+F_{edit}^{Norm}$ | [ğŸŒ Code](https://github.com/modelscope/facechain) 

  [ğŸ“„ Multi-Concept Customization of Text-to-Image Diffusion](https://arxiv.org/abs/2212.04488) | ğŸ“– CVPR 2023 | ğŸ”€ $F_{inv}^T+F_{edit}^{Norm}$ | [ğŸŒ Code](https://github.com/adobe-research/custom-diffusion) 

  [ğŸ“„ Cones: Concept Neurons in Diffusion Models for Customized Generation](https://arxiv.org/abs/2303.05125) | ğŸ“– ICML 2023 | ğŸ”€ $F_{inv}^T+F_{edit}^{Norm}$ | [ğŸŒ Code]() 
   
  [ğŸ“„ SVDiff: Compact Parameter Space for Diffusion Fine-Tuning](https://arxiv.org/abs/2303.11305) | ğŸ“– ICCV 2023 | ğŸ”€ $F_{inv}^T+F_{edit}^{Norm}$ | [ğŸŒ Code](https://github.com/mkshing/svdiff-pytorch) 


  [ğŸ“„ Low-Rank Adaptation for Fast Text-to-Image Diffusion Fine-Tuning]() | ğŸ“– | ğŸ”€ $F_{inv}^T+F_{edit}^{Norm}$| [ğŸŒ Code](https://github.com/cloneofsimo/lora) 

  [ğŸ“„ A Closer Look at Parameter-Efficient Tuning in Diffusion Models](https://arxiv.org/abs/2303.18181) | ğŸ“– Arxiv 2023 | ğŸ”€ $F_{inv}^T+F_{edit}^{Norm}$ | [ğŸŒ Code](https://github.com/Xiang-cd/unet-finetune) 
   
  <!-- [ğŸ“„ CatVersion]() | [ğŸ“– ] | [Inversion+Editing] | [ğŸŒ Code]()  -->


  [ğŸ“„ Break-a-scene: Extracting multiple concepts from a single image](https://arxiv.org/abs/2305.16311) | ğŸ“– SIGGRAPH 2023 | ğŸ”€ $F_{inv}^T+F_{edit}^{Norm}$ | [ğŸŒ Code](https://github.com/google/break-a-scene) 

  [ğŸ“„ Clic: Concept Learning in Context](https://arxiv.org/abs/2311.17083) | ğŸ“– Arxiv 2023 | ğŸ”€ $F_{inv}^T+F_{edit}^{Norm}$ | [ğŸŒ Code](https://mehdi0xc.github.io/clic/) 
   
  [ğŸ“„ Disenbooth: Disentangled parameter-efficient tuning for subject-driven text-to-image generation](https://arxiv.org/abs/2305.03374) | ğŸ“– Arxiv 2023 | ğŸ”€ $F_{inv}^T+F_{edit}^{Norm}$ | [ğŸŒ Code](https://github.com/forchchch/DisenBooth) 
  
  [ğŸ“„ Decoupled Textual Embeddings for Customized Image Generation](https://arxiv.org/abs/2312.11826) | ğŸ“– AAAI 2024 | ğŸ”€ $F_{inv}^T+F_{edit}^{Norm}$ | [ğŸŒ Code](https://github.com/PrototypeNx/DETEX) 

  [ğŸ“„ ViCo: Detail-Preserving Visual Condition for Personalized Text-to-Image Generation](https://arxiv.org/abs/2306.00971) | ğŸ“– Arxiv 2023 | ğŸ”€ $F_{inv}^T+F_{edit}^{Attn}$ | [ğŸŒ Code](https://github.com/haoosz/ViCo) 
   
  [ğŸ“„ DreamMatcher: Appearance Matching Self-Attention for Semantically-Consistent Text-to-Image Personalization](https://arxiv.org/abs/2402.09812) | ğŸ“– CVPR 2024 | ğŸ”€ $F_{inv}^T+F_{edit}^{Attn}$ | [ğŸŒ Code](https://ku-cvlab.github.io/DreamMatcher/) 


  [ğŸ“„ Pick-and-Draw: Training-free Semantic Guidance for Text-to-Image Personalization](https://arxiv.org/abs/2401.16762) | ğŸ“– Arxiv 2024 | ğŸ”€ $F_{inv}^F+F_{edit}^{Optim}$ | [ğŸŒ Code]() 
  

### 2. Training-Based Approaches

  [ğŸ“„ Taming Encoder for Zero Fine-tuning Image Customization with Text-to-Image Diffusion Models](https://arxiv.org/abs/2304.02642) | ğŸ“– ICLR 2024 | [ğŸŒ Code]() 

  [ğŸ“„ InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning](https://arxiv.org/abs/2304.03411) | ğŸ“– CVPR 2024] | [ğŸŒ Code](https://jshi31.github.io/InstantBooth/) 
   
  [ğŸ“„ Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models](https://arxiv.org/abs/2302.12228) | ğŸ“– Arxiv 2023 | [ğŸŒ Code](https://tuning-encoder.github.io/) 
  
  [ğŸ“„ Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach](https://arxiv.org/abs/2305.13579) | ğŸ“– ICLR 2024 | [ğŸŒ Code](https://github.com/drboog/ProFusion) 

  [ğŸ“„ FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention](https://arxiv.org/abs/2305.10431) | ğŸ“– Arxiv 2023 | [ğŸŒ Code]() 
   
  [ğŸ“„ PhotoMaker: Customizing Realistic Human Photos via Stacked {ID} Embedding](https://arxiv.org/abs/2312.04461) | ğŸ“– Arxiv 2023 |  [ğŸŒ Code](https://github.com/TencentARC/PhotoMaker) 

  [ğŸ“„ PhotoVerse: Tuning-Free Image Customization with Text-to-Image Diffusion Models](https://arxiv.org/abs/2309.05793) | ğŸ“– Arxiv 2023 | [ğŸŒ Code](https://photoverse2d.github.io/) 


  [ğŸ“„ InstantID: Zero-shot Identity-Preserving Generation in Seconds](https://arxiv.org/abs/2401.07519) | ğŸ“– Arxiv 2024 | [ğŸŒ Code](https://github.com/InstantID/InstantID) 

  [ğŸ“„ ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation](https://arxiv.org/abs/2302.13848) | ğŸ“– ICCV 2023 | [ğŸŒ Code](https://github.com/csyxwei/ELITE) 
   
  [ğŸ“„ BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing](https://arxiv.org/abs/2305.14720) | ğŸ“– NeurIPS 2023 | [ğŸŒ Code]() 
  
  [ğŸ“„ Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models](https://arxiv.org/abs/2307.06925) | ğŸ“– SIGGRAPH 2023 | [ğŸŒ Code](https://arxiv.org/abs/2305.14720) 

  [ğŸ“„ Unified Multi-Modal Latent Diffusion for Joint Subject and Text Conditional Image Generation](https://arxiv.org/abs/2303.09319) | ğŸ“– Arxiv 2023 | [ğŸŒ Code]() 
   
  [ğŸ“„ Subject-Diffusion: Open Domain Personalized Text-to-Image Generation without Test-time Fine-tuning](https://arxiv.org/abs/2307.11410) | ğŸ“– Arxiv 2023 | [ğŸŒ Code]() 

  [ğŸ“„ Instruct-Imagen: Image Generation with Multi-modal Instruction](https://arxiv.org/abs/2401.01952) | ğŸ“– Arxiv 2024 | [ğŸŒ Code]() 



<br>


## Attribute-Driven Customization:
### 1. Training-Free Approaches

  [ğŸ“„ ProSpect: Prompt Spectrum for Attribute-Aware Personalization of Diffusion Models](https://arxiv.org/abs/2305.16225) | ğŸ“– Arxiv 2023 | ğŸ”€ $F_{inv}^T+F_{edit}^{Norm}$ | [ğŸŒ Code](https://github.com/zyxElsa/ProSpect) 

  [ğŸ“„ An Image is Worth Multiple Words: Multi-attribute Inversion for Constrained Text-to-Image Synthesis](https://arxiv.org/abs/2311.11919) | ğŸ“– Arxiv 2023 | ğŸ”€ $F_{inv}^T+F_{edit}^{Norm}$ | [ğŸŒ Code]() 
   
  [ğŸ“„ Concept Decomposition for Visual Exploration and Inspiration](https://arxiv.org/abs/2305.18203) | ğŸ“– TOG 2023 | ğŸ”€ $F_{inv}^T+F_{edit}^{Norm}$ | [ğŸŒ Code](https://github.com/google/inspiration_tree) 
  
  [ğŸ“„ ReVersion: Diffusion-Based Relation Inversion from Images](https://arxiv.org/abs/2303.13495) | ğŸ“– Arxiv 2023 | ğŸ”€ $F_{inv}^T+F_{edit}^{Norm}$ | [ğŸŒ Code](https://github.com/ziqihuangg/ReVersion) 

  [ğŸ“„ Learning Disentangled Identifiers for Action-Customized Text-to-Image Generation](https://arxiv.org/abs/2311.15841) | ğŸ“– Arxiv 2023 | ğŸ”€ $F_{inv}^T+F_{edit}^{Norm}$ | [ğŸŒ Code](https://adi-t2i.github.io/ADI/) 
   
  [ğŸ“„ Lego: Learning to Disentangle and Invert Concepts Beyond Object Appearance in Text-to-Image Diffusion Models](https://arxiv.org/abs/2311.13833) | ğŸ“– Arxiv 2023 | ğŸ”€ $F_{inv}^T+F_{edit}^{Norm}$ | [ğŸŒ Code]() 

  [ğŸ“„ StyleDrop: Text-to-Image Generation in Any Style](https://arxiv.org/abs/2306.00983) | ğŸ“– NeurIPS 2023 | ğŸ”€ $F_{inv}^T+F_{edit}^{Norm}$ | [ğŸŒ Code](https://styledrop.github.io/) 






### 2. Training-Based Approaches
  [ğŸ“„ ArtAdapter: Text-to-Image Style Transfer using Multi-Level Style Encoder and Explicit Adaptation](https://arxiv.org/abs/2312.02109) | ğŸ“– Arxiv 2023 | [ğŸŒ Code](https://github.com/cardinalblue/ArtAdapter) 

  [ğŸ“„ DreamCreature: Crafting Photorealistic Virtual Creatures from Imagination](https://arxiv.org/abs/2311.15477) | ğŸ“– Arxiv 2023 | [ğŸŒ Code]() 
   
  [ğŸ“„ Language-Informed Visual Concept Learning](https://arxiv.org/abs/2312.03587) | ğŸ“– ICLR 2024| [ğŸŒ Code](https://cs.stanford.edu/~yzzhang/projects/concept-axes/) 

  [ğŸ“„ pOps: Photo-Inspired Diffusion Operators](https://arxiv.org/abs/2406.01300) | ğŸ“– Arxiv 2024 | [ğŸŒ Code](https://github.com/pOpsPaper/pOps) 